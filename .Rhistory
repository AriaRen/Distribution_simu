plot(reg.summary$bic,xlab="Number of Predictors", ylab = "BIC", type = "l")
which.min(reg.summary$bic)
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
par(mfrow=c(1,1))
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
coef(regfit.full,6)
coef(regfit.full,8)
regfit.fwd = regsubsets(Salary~.,data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
regfit.bwd = regsubsets(Salary~., data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)
set.seed(1)
train = sample(c(TRUE,FALSE),nrow(Hitters),rep=T)
train
summary(train)
test = (!train)
test
Summary(!train)
Summary(test)
summary(test)
summary(train)
regfit.best = regsubsets(Salary~.,data = Hitters[train,],nvmax = 19)
test.mat = model.matrix(Salary~.,data=Hitters[test,])
test.mat
val.errors = rep(NA,19)
val.errors
coefi = coef(regfit.best, id = 1)
coefi
names(coefi)
test.mat[,names(coefi)]
library ISLR
library(ISLR)
library(glmnet)
install.packages('Matrix')
library(ISLR)
data(College)
set.seed(1)
train = sample(1:dim(College)[1], dim(College)[1] / 2)
train
test = -train
College.train = College[train, ]
College.test = College[test, ]
fit.lm = lm(Apps ~ ., data = College.train)
pred.lm = predict(fit.lm, College.test)
mean((pred.lm - College.test$Apps)^2)
train.mat <- model.matrix(Apps ~ ., data = College.train)
test.mat <- model.matrix(Apps ~ ., data = College.test)
grid <- 10 ^ seq(4, -2, length = 100)
fit.ridge <- glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlam.ridge <- cv.ridge$lambda.min
bestlam.ridge
library(glmnet)
library(Matrix)
library(foreach)
library(glmnet)
train.mat <- model.matrix(Apps ~ ., data = College.train)
test.mat <- model.matrix(Apps ~ ., data = College.test)
grid <- 10 ^ seq(4, -2, length = 100)
fit.ridge <- glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
cv.ridge <- cv.glmnet(train.mat, College.train$Apps, alpha = 0, lambda = grid, thresh = 1e-12)
bestlam.ridge <- cv.ridge$lambda.min
bestlam.ridge
pred.ridge <- predict(fit.ridge, s = bestlam.ridge, newx = test.mat)
mean((pred.ridge - College.test$Apps)^2)
mean((pred.lm - College.test$Apps)^2)
fit.lasso = glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grad, thresh = 1e-12)
cv.lasso <- cv.glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
fit.lasso = glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
cv.lasso <- cv.glmnet(train.mat, College.train$Apps, alpha = 1, lambda = grid, thresh = 1e-12)
bestlam.lasso <- cv.lasso$lambda.min
bestlam.lasso
pred.lasso <- predict(fit.lasso, s = bestlam.lasso, newx = test.mat)
mean((pred.lasso - College.test$Apps)^2)
predict(fit.lasso, s = bestlam.lasso, type = "coefficients")
library(ISLR)
attach(Auto)
mpg01 <- rep(0, length(mpg))
mpg01[mpg > median(mpg)] <- 1
Auto <- data.frame(Auto, mpg01)
cor(Auto[, -9])
pairs(Auto)
boxplot(cylinders ~ mpg01, data = Auto, main = "Cylinders ~ mpg01")
boxplot(acceleration ~ mpg01, data = Auto, main = "Acceleration ～ mpg01")
boxplot(acceleration ~ mpg01, data = Auto, main = "Acceleration vs mpg01")
train <- (year %% 2 == 0)
Auto.train <- Auto[train, ]
Auto.test <- Auto[!train, ]
mpg01.test <- mpg01[!train]
fit.lda <- lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, subset = train)
fit.lda
library(leaps)
fit.lda <- lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, subset = train)
library(MASS)
fit.lda <- lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, subset = train)
fit.lda
pred.lda <- predict(fit.lda, Auto.test)
table(pred.lda$class, mpg01.test)
pred.lda <- predict(fit.lda, Auto.test)
table(pred.lda$class, mpg01.test)
mean(pred.lda$class != mpg01.test)
fit.qda <- qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, subset = train)
fit.qda
table(pred.lda$class, mpg01.test)
mean(pred.lda$class != mpg01.test)
pred.qda <- predict(fit.qda, Auto.test)
table(pred.qda$class, mpg01.test)
mean(pred.qda$class != mpg01.test)
fit.glm <- glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto, family = binomial, subset = train)
summary(fit.glm)
mean(pred.glm != mpg01.test)
probs <- predict(fit.glm, Auto.test, type = "response")
pred.glm <- rep(0, length(probs))
pred.glm[probs > 0.5] <- 1
table(pred.glm, mpg01.test)
mean(pred.glm != mpg01.test)
train.X <- cbind(cylinders, weight, displacement, horsepower)[train, ]
test.X <- cbind(cylinders, weight, displacement, horsepower)[!train, ]
train.mpg01 <- mpg01[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.mpg01, k = 1)
table(pred.knn, mpg01.test)
library(kknn)
train.X <- cbind(cylinders, weight, displacement, horsepower)[train, ]
test.X <- cbind(cylinders, weight, displacement, horsepower)[!train, ]
train.mpg01 <- mpg01[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.mpg01, k = 1)
table(pred.knn, mpg01.test)
library(class)
train.X <- cbind(cylinders, weight, displacement, horsepower)[train, ]
test.X <- cbind(cylinders, weight, displacement, horsepower)[!train, ]
train.mpg01 <- mpg01[train]
set.seed(1)
pred.knn <- knn(train.X, test.X, train.mpg01, k = 1)
table(pred.knn, mpg01.test)
mean(pred.knn != mpg01.test)
pred.knn <- knn(train.X, test.X, train.mpg01, k = 10)
table(pred.knn, mpg01.test)
mean(pred.knn != mpg01.test)
pred.knn <- knn(train.X, test.X, train.mpg01, k = 100)
table(pred.knn, mpg01.test)
mean(pred.knn != mpg01.test)
library(leaps)
dataframe = data.frame(y=y, x=x)
regfit = regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) +
I(x^9) + I(x^10), data = dataframe, nvmax = 10)
reg.summary = summary(regfit)
par(mfrow = c(2,2))
plot(reg.summary$cp, xlab = "Number of variables", ylab = "Cp", type = "l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch = 20)
plot(reg.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", cex = 2, pch = 20)
set.seed(1)
x <- rnorm(100)
noise <- rnorm(100)
#b
b0 <- 2
b1 <- 3
b2 <- 4
b3 <- 1
y <- b0 + b1 * x + b2 * x^2 + b3 * x^3 + noise
#c
library(leaps)
dataframe = data.frame(y=y, x=x)
regfit = regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) +
I(x^9) + I(x^10), data = dataframe, nvmax = 10)
reg.summary = summary(regfit)
par(mfrow = c(2,2))
plot(reg.summary$cp, xlab = "Number of variables", ylab = "Cp", type = "l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch = 20)
plot(reg.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", cex = 2, pch = 20)
set.seed(1)
x <- rnorm(100)
noise <- rnorm(100)
#b
b0 <- 2
b1 <- 3
b2 <- -1
b3 <- 0.5
y <- b0 + b1 * x + b2 * x^2 + b3 * x^3 + noise
library(leaps)
dataframe = data.frame(y=y, x=x)
regfit = regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) +
I(x^9) + I(x^10), data = dataframe, nvmax = 10)
reg.summary = summary(regfit)
par(mfrow = c(2,2))
plot(reg.summary$cp, xlab = "Number of variables", ylab = "Cp", type = "l")
points(which.min(reg.summary$cp), reg.summary$cp[which.min(reg.summary$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(reg.summary$bic), reg.summary$bic[which.min(reg.summary$bic)], col = "red", cex = 2, pch = 20)
plot(reg.summary$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(reg.summary$adjr2), reg.summary$adjr2[which.max(reg.summary$adjr2)], col = "red", cex = 2, pch = 20)
regfit.fwd <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = dataframe, nvmax = 10, method = "forward")
reg.summary.fwd <- summary(regfit.fwd)
par(mfrow = c(2, 2))
plot(reg.summary.fwd$cp, xlab = "Number of variables", ylab = "C_p", type = "l")
points(which.min(reg.summary.fwd$cp), reg.summary.fwd$cp[which.min(reg.summary.fwd$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary.fwd$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(reg.summary.fwd$bic), reg.summary.fwd$bic[which.min(reg.summary.fwd$bic)], col = "red", cex = 2, pch = 20)
plot(reg.summary.fwd$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(reg.summary.fwd$adjr2), reg.summary.fwd$adjr2[which.max(reg.summary.fwd$adjr2)], col = "red", cex = 2, pch = 20)
coef(regfit.fwd, which.max(reg.summary.fwd$adjr2))
regfit.bwd <- regsubsets(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), data = dataframe, nvmax = 10, method = "backward")
reg.summary.bwd <- summary(regfit.bwd)
par(mfrow = c(2, 2))
plot(reg.summary.bwd$cp, xlab = "Number of variables", ylab = "C_p", type = "l")
points(which.min(reg.summary.bwd$cp), reg.summary.bwd$cp[which.min(reg.summary.bwd$cp)], col = "red", cex = 2, pch = 20)
plot(reg.summary.bwd$bic, xlab = "Number of variables", ylab = "BIC", type = "l")
points(which.min(reg.summary.bwd$bic), reg.summary.bwd$bic[which.min(reg.summary.bwd$bic)], col = "red", cex = 2, pch = 20)
plot(reg.summary.bwd$adjr2, xlab = "Number of variables", ylab = "Adjusted R^2", type = "l")
points(which.max(reg.summary.bwd$adjr2), reg.summary.bwd$adjr2[which.max(reg.summary.bwd$adjr2)], col = "red", cex = 2, pch = 20)
library(glmnet)
new_matrix = model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6) +
I(x^7) + I(x^8) + I(x^9) + I(x^10), data = dataframe)[, -1]
cv.lasso = cv.glmnet(new_matrix, y, alpha = 1)
plot(cv.lasso)
bestlam <- cv.lasso$lambda.min
bestlam
fit.lasso <- glmnet(new_matrix, y, alpha = 1)
predict(fit.lasso, s = bestlam, type = "coefficients")[1:11, ]
set.seed(1)
x=matrix(rnorm(20*2),ncol=2)
x
y = c(rep(-1,10),rep(1,10))
y
x[y==1,]=x[y==1,]+1
x
plot(x,col=(3-y))
dat = data.frame(x=x, y = as.factor(y))
library(e1071)
dat
svmfit=svm(y~.,data = dat, kernel = "linear", cost=10,scale = FALSE)
plot(svmfit, dat)
svmfit$index
summary(svmfit)
svmfit=svm(y~.,data = dat, kernel = "linear", cost=0.1, scale = FALSE)
plot(svmfit, dat)
svmfit$index
svmfit = svm(y~., data = dat[train,], kernel ="radial", gamma = 1, cost = 1e5)
plot(svmfit, dat[train,])
set.seed(1)
x=matrix(rnorm(20*2),ncol=2)
x
y = c(rep(-1,10),rep(1,10))
y
x[y==1,]=x[y==1,]+1
x
plot(x,col=(3-y))
set.seed(1)
tune.out = tune(svm,y~.,data = dat, kernel = "linear", ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
bestmod = tune.out$best.model
tune.out
summary(bestmod)
xtest = matrix(rnorm(20*2), ncol=2)
xtest
ytest = sample(c(-1,1), rep = TRUE)
ytest
ytest = sample(c(-1,1), 20，rep = TRUE)
ytest = sample(c(-1,1), 20, rep = TRUE)
xtest[ytest==1,] = xtest[ytest==1,] + 1
testdat = data.frame(x=xtest, y=as.factor(ytest))
ypred=predict(bestmod,testdat)
table(predict=ypred,truth=testdat$y)
svmfit=svm(y~.,data = dat, kernel = "linear", cost =0.01, scale = FALSE)
ypred = predict(svmfit,testdat)
table(predict = ypred, truth = testdat$y)
x[y==1,] = x[y==1,]+0.5
plot(x, col=(y+5)/2,pch=19)
dat = data.frame(x=x,y=as.factor(y))
svmfit=svm(y~., data = dat, kernel = "linear", cost = 1e5)
summary(svmfit)
plot(svmfit,dat)
svmfit$index
set.seed(1)
x = matrix(rnorm(200*2), ncol = 2)
x[1:100,] = x [1:100,] + 2
x[101:150,] = x[101:150,] -2
y = c(rep(1,150),rep(2,50))
dat = data.frame(x = x, y = as.factor(y))
plot(x, col=y)
train = sample(200, 100)
train
sample(5,5)
plot(svmfit, dat[train,])
svmfit = svm(y~., data = dat[train,], kernel ="radial", gamma = 1, cost = 1e5)
plot(svmfit, dat[train,])
set.seed(1)
svmfit = svm(y~., data = dat[train,], kernel ="radial", gamma = 1, cost = 1)
plot(svmfit, dat[train,])
summary(svmfit)
svmfit = svm(y~., data = dat[train,], kernel ="radial", gamma = 1, cost = 1e5)
plot(svmfit, dat[train,])
set.seed(1)
tune.out = tune(svm, y~., data = dat[train,], kernel = "radial", ranges=list(cost=c(0.1,1,10,100,1000), gamma=c(0.5,1,2,3,4)))
summary(tune.out)
table(true = dat[-train,"y"], predicted = predict(tune.out$best.model, newdata = dat[-train,]))
linear.mse = matrix(rep(0,24*8), 8, 24)
for (n in 1:8) {
for (i in 1:24) {
linear.mse[n,i] = mean((predict(lm(Store.train(n,i)[,97]~.,
data = Store.train(n,i)[, 1:96]),
data.frame(Store.test(n,i)[,1:97]),interval="confidence")[1]-Store.test(n,i)[97])^2)
}
}
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, 2), ylim = c(-3, 3), xlab = "X1", ylab = "X2")
# X2 < 1
lines(x = c(-2, 2), y = c(1, 1))
# X1 < 1 with X2 < 1
lines(x = c(1, 1), y = c(-3, 1))
text(x = (-2 + 1)/2, y = -1, labels = c(-1.8))
text(x = 1.5, y = -1, labels = c(0.63))
# X2 < 2 with X2 >= 1
lines(x = c(-2, 2), y = c(2, 2))
text(x = 0, y = 2.5, labels = c(2.49))
# X1 < 0 with X2<2 and X2>=1
lines(x = c(0, 0), y = c(1, 2))
text(x = -1, y = 1.5, labels = c(-1.06))
text(x = 1, y = 1.5, labels = c(0.21))
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, -2), ylim = c(-3, -3), xlab = "X1", ylab = "X2")
lines(x = c(-2, 2), y = c(1,1))
lines(x = c(1, 1), y = c(-3, 1))
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, 2), ylim = c(-3, 3), xlab = "X1", ylab = "X2")
# X2 < 1
lines(x = c(-2, 2), y = c(1, 1))
# X1 < 1 with X2 < 1
lines(x = c(1, 1), y = c(-3, 1))
text(x = (-2 + 1)/2, y = -1, labels = c(-1.8))
text(x = 1.5, y = -1, labels = c(0.63))
# X2 < 2 with X2 >= 1
lines(x = c(-2, 2), y = c(2, 2))
text(x = 0, y = 2.5, labels = c(2.49))
# X1 < 0 with X2<2 and X2>=1
lines(x = c(0, 0), y = c(1, 2))
text(x = -1, y = 1.5, labels = c(-1.06))
text(x = 1, y = 1.5, labels = c(0.21))
par(xpd = NA)
plot(NA, NA, type = "n", xlim = c(-2, 2), ylim = c(-3, 3), xlab = "X1", ylab = "X2")
lines(x = c(-2, 2), y = c(1, 1))
lines(x = c(1, 1), y = c(-3, 1))
text(x = (-2 + 1)/2, y = -1, labels = c(-1.8))
text(x = 1.5, y = -1, labels = c(0.63))
lines(x = c(-2, 2), y = c(2, 2))
text(x = 0, y = 2.5, labels = c(2.49))
lines(x = c(0, 0), y = c(1, 2))
text(x = -1, y = 1.5, labels = c(-1.06))
text(x = 1, y = 1.5, labels = c(0.21))
library(ISLR)
set.seed(1)
train <- sample(1:nrow(Carseats), nrow(Carseats) / 2)
Carseats.train <- Carseats[train, ]
Carseats.test <- Carseats[-train, ]
library(tree)
tree.carseats <- tree(Sales ~ ., data = Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
yhat <- predict(tree.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
cv.carseats <- cv.tree(tree.carseats)
plot(cv.carseats$size, cv.carseats$dev, type = "b")
tree.min <- which.min(cv.carseats$dev)
points(tree.min, cv.carseats$dev[tree.min], col = "red", cex = 2, pch = 20)
prune.carseats <- prune.tree(tree.carseats, best = 8)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
yhat <- predict(prune.carseats, newdata = Carseats.test)
mean((yhat - Carseats.test$Sales)^2)
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
library(randomForest)
library(randomForest)
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
importance(bag.carseats)
bag.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500, importance = TRUE)
yhat.bag <- predict(bag.carseats, newdata = Carseats.test)
mean((yhat.bag - Carseats.test$Sales)^2)
rf.carseats <- randomForest(Sales ~ ., data = Carseats.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf <- predict(rf.carseats, newdata = Carseats.test)
mean((yhat.rf - Carseats.test$Sales)^2)
importance(rf.carseats)
library(ISLR)
var <- ifelse(Auto$mpg > median(Auto$mpg), 1, 0)
Auto$mpglevel <- as.factor(var)
var
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "linear", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
library(e1071)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "linear", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100, 1000)))
summary(tune.out)
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2, 3, 4)))
summary(tune.out)
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2, 3, 4)))
summary(tune.out)
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2, 3, 4)))
summary(tune.out)
set.seed(1)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
tune.out <- tune(svm, mpglevel ~ ., data = Auto, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
plot(hclust(d, method = "complete"), labels = c(2,1,4,3))
set.seed(2)
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete)
set.seed(1)
hc.complete <- hclust(dist(USArrests), method = "complete")
plot(hc.complete)
cutree(hc.complete, 3)
sd.data <- scale(USArrests)
hc.complete.sd <- hclust(dist(sd.data), method = "complete")
plot(hc.complete.sd)
cutree(hc.complete.sd, 3)
table(cutree(hc.complete, 3), cutree(hc.complete.sd, 3))
setwd("~/Work_Hard+++/500W Causal Inference/Lab 1/Data")
read.csv("ads_effect.csv")
summary(ad)
ad = read.csv("ads_effect.csv")
View(ad)
summary(ad)
hist(cust_id)
names(ad)
cust_id
attach(ad)
hist(cust_id)
hist(income)
hist(female)
hist(buy)
plot(female, buy)
cor(female, buy)
cor(, buy)
cor(:, buy)
cor(ad)
lm.fit(buy~ad, data = ad)
lm.fit(buy~ad, data = ad)
lm.fit(buy~ad$ad, data = ad)
lm.fit(buy~ad)
lm(buy~ad, data = ad)
hist(buy)
buy_ad = lm(buy~ad, data = ad)
summary(buy_ad)
setwd("~/WUCCI/distribution")
library(spatstat)
runifpoint(50)
read.csv('newdata.csv')
data <- read.csv('newdata.csv')
Kest(data)
head(data)
data[1:30, :]
data[1:30, ]
ppp(data[,1], data[,2], data[,3], c(-20, 20), c(-20, 20), c(-20, 20))
ppp(data[,1], data[,2], data[,3])
help ppp
?ppp
3Dplot(data)
3dplot()
?3d
?3D
?3Dplot
ppp(data[,1],data[,2],c(-20,20),c(-20,20))
A1 <- ppp(data[,1],data[,2],c(-20,20),c(-20,20))
plot(A1)
Kest(A1)
?p-p
rpoispp3(0)
rpoispp3(20)
spp3(data)
rpoispp3(data)
?rpoispp3()
rpoisppe(20)
rpoispp3(20)
X =rpoispp3(20)
K3est(X)
A2 = K3est(X)
plot(A2)
plot(X)
rpoispp3(data)
?pp3
pp3(data[,1], data[,2], data[,3])
pp3(data[,1], data[,2], data[,3], box3(c(-20,20)))
3Ddata <- pp3(data[,1], data[,2], data[,3], box3(c(-20,20)))
ThreeD <- pp3(data[,1], data[,2], data[,3], box3(c(-20,20)))
K3est(ThreeD)
data <- read.csv('newdata.csv')
ThreeD <- pp3(data[,1], data[,2], data[,3], box3(c(-20,20)))
K <- K3est(ThreeD)
plot(K)
